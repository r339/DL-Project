{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V6E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73f2LwjC6g-J",
        "outputId": "d011f526-d906-41ee-cab6-0372a18874fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset '/content/diabetes_dataset.csv' loaded successfully.\n",
            "\n",
            "--- Training and Calibrating the Model ---\n",
            "Uncalibrated model trained.\n",
            "Model calibration complete.\n",
            "\n",
            "--- Comparative Evaluation ---\n",
            "\n",
            "--- Performance (Accuracy, etc.) ---\n",
            "\n",
            "[Uncalibrated Model]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " No Diabetes       0.83      1.00      0.91      8001\n",
            "    Diabetes       1.00      0.87      0.93     11999\n",
            "\n",
            "    accuracy                           0.92     20000\n",
            "   macro avg       0.92      0.93      0.92     20000\n",
            "weighted avg       0.93      0.92      0.92     20000\n",
            "\n",
            "\n",
            "[Calibrated Model]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            " No Diabetes       0.83      1.00      0.91      8001\n",
            "    Diabetes       1.00      0.87      0.93     11999\n",
            "\n",
            "    accuracy                           0.92     20000\n",
            "   macro avg       0.92      0.93      0.92     20000\n",
            "weighted avg       0.93      0.92      0.92     20000\n",
            "\n",
            "\n",
            "--- Calibration Quality (Lower is Better) ---\n",
            "Brier Score (Uncalibrated): 0.0663\n",
            "Brier Score (Calibrated):   0.0663\n",
            "\n",
            "Expected Cal. Error (Uncalibrated): 0.0042\n",
            "Expected Cal. Error (Calibrated):   0.0041\n",
            "\n",
            "--- Writing Audit Logs for Triage Comparison ---\n",
            "Audit logs successfully written to 'audit_logs/predictions_calibration.jsonl'.\n",
            "\n",
            "--- Workflow Complete ---\n"
          ]
        }
      ],
      "source": [
        "# DL_final_Calibration.py\n",
        "# Approach: Uncertainty and Risk-Aware Triage using Probability Calibration.\n",
        "# This script trains, evaluates, and compares an uncalibrated model against a calibrated\n",
        "# model to demonstrate the improvement in probability reliability for risk assessment.\n",
        "# It uses append-only audit logs for accountability and traceability.\n",
        "\n",
        "# Env: Python 3.10+ recommended. scikit-learn >=1.1, pandas, numpy.\n",
        "\n",
        "import os, json, time, uuid, platform\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, accuracy_score, precision_recall_fscore_support, brier_score_loss, classification_report\n",
        ")\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "# ================== CONFIG ==================\n",
        "# Ensure this CSV file is in the same directory as the script\n",
        "csv_path = \"/content/diabetes_dataset.csv\"\n",
        "model_version = \"v3.0-calibration\"\n",
        "risk_threshold_high = 0.75  # Calibrated probability threshold for high-risk flag\n",
        "risk_threshold_low = 0.25   # Calibrated probability threshold for low-risk flag\n",
        "random_seed = 42\n",
        "\n",
        "# ================== SETUP ==================\n",
        "# Create a unique ID for this execution run for traceability\n",
        "run_id = str(uuid.uuid4())\n",
        "timestamp = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
        "\n",
        "# Create directory for audit logs if it doesn't exist\n",
        "os.makedirs(\"audit_logs\", exist_ok=True)\n",
        "\n",
        "# Helper function to calculate Expected Calibration Error (ECE)\n",
        "def expected_calibration_error(y_true, y_prob, n_bins=15):\n",
        "    \"\"\"A simple function to calculate ECE.\"\"\"\n",
        "    bin_limits = np.linspace(0, 1, n_bins + 1)\n",
        "    bin_lowers, bin_uppers = bin_limits[:-1], bin_limits[1:]\n",
        "    ece = 0\n",
        "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "        in_bin = (y_prob > bin_lower) & (y_prob <= bin_upper)\n",
        "        prop_in_bin = np.mean(in_bin)\n",
        "        if prop_in_bin > 0:\n",
        "            accuracy_in_bin = np.mean(y_true[in_bin])\n",
        "            avg_confidence_in_bin = np.mean(y_prob[in_bin])\n",
        "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "    return ece\n",
        "\n",
        "# ================== DATA PREPARATION ==================\n",
        "try:\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(f\"Dataset '{csv_path}' loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"CRITICAL ERROR: The file '{csv_path}' was not found. Please check the path.\")\n",
        "    exit()\n",
        "\n",
        "# Define target and features\n",
        "target_col = 'diagnosed_diabetes'\n",
        "# Drop rows with NaN in the target column before splitting\n",
        "df.dropna(subset=[target_col], inplace=True)\n",
        "# Drop the target and the closely related 'diabetes_stage' column to prevent data leakage\n",
        "feature_cols = df.drop(columns=[target_col, 'diabetes_stage']).columns\n",
        "\n",
        "X = df[feature_cols]\n",
        "y = df[target_col]\n",
        "\n",
        "# Split data: 60% train, 20% calibration, 20% test\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=random_seed, stratify=y)\n",
        "X_calib, X_test, y_calib, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=random_seed, stratify=y_temp)\n",
        "\n",
        "# Identify column types for preprocessing\n",
        "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
        "numerical_features = X.select_dtypes(include=['number']).columns\n",
        "\n",
        "# Create the preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# ================== MODELING & CALIBRATION ==================\n",
        "print(\"\\n--- Training and Calibrating the Model ---\")\n",
        "\n",
        "# Define a base model known to benefit from calibration (e.g., Gradient Boosting)\n",
        "base_classifier = GradientBoostingClassifier(n_estimators=100, random_state=random_seed)\n",
        "\n",
        "# 1. Create and train the UNCALIBRATED model pipeline\n",
        "uncalibrated_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', base_classifier)\n",
        "])\n",
        "uncalibrated_pipeline.fit(X_train, y_train)\n",
        "print(\"Uncalibrated model trained.\")\n",
        "\n",
        "# 2. Create and train the CALIBRATED model\n",
        "# This wraps the already-trained classifier and learns a correction function\n",
        "calibrated_classifier = CalibratedClassifierCV(\n",
        "    estimator=uncalibrated_pipeline.named_steps['classifier'], # Changed base_estimator to estimator\n",
        "    method='isotonic', # A powerful non-parametric calibration method\n",
        "    cv='prefit'\n",
        ")\n",
        "# Pre-process the calibration data before fitting the calibrator\n",
        "X_calib_processed = uncalibrated_pipeline.named_steps['preprocessor'].transform(X_calib)\n",
        "calibrated_classifier.fit(X_calib_processed, y_calib)\n",
        "print(\"Model calibration complete.\")\n",
        "\n",
        "# ================== EVALUATION ==================\n",
        "print(\"\\n--- Comparative Evaluation ---\")\n",
        "\n",
        "# Get predictions and probabilities from both models\n",
        "y_pred_uncalib = uncalibrated_pipeline.predict(X_test)\n",
        "prob_uncalib = uncalibrated_pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "X_test_processed = uncalibrated_pipeline.named_steps['preprocessor'].transform(X_test)\n",
        "y_pred_calib = calibrated_classifier.predict(X_test_processed)\n",
        "prob_calib = calibrated_classifier.predict_proba(X_test_processed)[:, 1]\n",
        "\n",
        "# --- Performance Metrics ---\n",
        "print(\"\\n--- Performance (Accuracy, etc.) ---\")\n",
        "print(\"\\n[Uncalibrated Model]\")\n",
        "print(classification_report(y_test, y_pred_uncalib, target_names=[\"No Diabetes\", \"Diabetes\"]))\n",
        "print(\"\\n[Calibrated Model]\")\n",
        "print(classification_report(y_test, y_pred_calib, target_names=[\"No Diabetes\", \"Diabetes\"]))\n",
        "\n",
        "\n",
        "# --- Calibration Metrics (Lower is better) ---\n",
        "print(\"\\n--- Calibration Quality (Lower is Better) ---\")\n",
        "brier_uncalib = brier_score_loss(y_test, prob_uncalib)\n",
        "brier_calib = brier_score_loss(y_test, prob_calib)\n",
        "ece_uncalib = expected_calibration_error(y_test.values, prob_uncalib)\n",
        "ece_calib = expected_calibration_error(y_test.values, prob_calib)\n",
        "\n",
        "print(f\"Brier Score (Uncalibrated): {brier_uncalib:.4f}\")\n",
        "print(f\"Brier Score (Calibrated):   {brier_calib:.4f}\")\n",
        "print(f\"\\nExpected Cal. Error (Uncalibrated): {ece_uncalib:.4f}\")\n",
        "print(f\"Expected Cal. Error (Calibrated):   {ece_calib:.4f}\")\n",
        "\n",
        "\n",
        "# ================== AUDIT LOGGING ==================\n",
        "print(\"\\n--- Writing Audit Logs for Triage Comparison ---\")\n",
        "\n",
        "# Persist environment header for accountability\n",
        "run_header = {\n",
        "    \"run_id\": run_id,\n",
        "    \"timestamp_utc\": timestamp,\n",
        "    \"env\": {\n",
        "        \"python\": platform.python_version(),\n",
        "        \"sklearn\": sklearn.__version__,\n",
        "    },\n",
        "    \"model\": {\n",
        "        \"type\": \"ProbabilityCalibrationComparison\",\n",
        "        \"version\": model_version,\n",
        "        \"base_classifier\": type(base_classifier).__name__,\n",
        "        \"calibration_method\": \"isotonic\"\n",
        "    },\n",
        "    \"risk_thresholds\": {\"high\": risk_threshold_high, \"low\": risk_threshold_low},\n",
        "    \"target_col\": target_col\n",
        "}\n",
        "with open(\"audit_logs/run_header_calibration.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(run_header, f, indent=2)\n",
        "\n",
        "# Write a log for each prediction, comparing calibrated vs. uncalibrated outputs\n",
        "log_path = f\"audit_logs/predictions_calibration.jsonl\"\n",
        "with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for i in range(len(X_test)):\n",
        "        p_uncalib = prob_uncalib[i]\n",
        "        p_calib = prob_calib[i]\n",
        "\n",
        "        # Assign a risk flag based on the TRUSTED (calibrated) probability\n",
        "        if p_calib >= risk_threshold_high:\n",
        "            risk_assessment = \"HIGH_RISK\"\n",
        "        elif p_calib <= risk_threshold_low:\n",
        "            risk_assessment = \"LOW_RISK\"\n",
        "        else:\n",
        "            risk_assessment = \"UNCERTAIN_TRIAGE\" # Case for manual review\n",
        "\n",
        "        record = {\n",
        "            \"run_id\": run_id,\n",
        "            \"record_index\": int(X_test.index[i]), # Convert int64 to int\n",
        "            \"true_label\": int(y_test.iloc[i]),\n",
        "            \"uncalibrated_prob\": float(p_uncalib),\n",
        "            \"calibrated_prob\": float(p_calib),\n",
        "            \"calibration_impact\": float(p_calib - p_uncalib),\n",
        "            \"risk_assessment\": risk_assessment\n",
        "        }\n",
        "        f.write(json.dumps(record) + \"\\n\")\n",
        "\n",
        "print(f\"Audit logs successfully written to '{log_path}'.\")\n",
        "print(\"\\n--- Workflow Complete ---\")"
      ]
    }
  ]
}